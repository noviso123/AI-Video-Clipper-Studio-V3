"""
M√≥dulo de Narra√ß√£o Autom√°tica
Detecta sil√™ncios no v√≠deo e adiciona narra√ß√£o automaticamente.

Funcionalidades:
- Detec√ß√£o de sil√™ncios
- Gera√ß√£o de texto para narra√ß√£o baseado no contexto
- S√≠ntese de voz (TTS)
- Mixagem de √°udio
"""
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import numpy as np
from dataclasses import dataclass
from ..core.config import Config
from ..core.logger import setup_logger

logger = setup_logger(__name__)


@dataclass
class SilenceSegment:
    """Segmento de sil√™ncio detectado."""
    start: float
    end: float
    duration: float
    context_before: str = ""
    context_after: str = ""
    suggested_narration: str = ""


class SilenceDetector:
    """Detecta sil√™ncios no √°udio do v√≠deo."""
    
    def __init__(self, silence_threshold: float = 0.01, min_silence_duration: float = 1.0):
        """
        Args:
            silence_threshold: Limiar de volume para considerar sil√™ncio (0-1)
            min_silence_duration: Dura√ß√£o m√≠nima em segundos para considerar sil√™ncio
        """
        self.silence_threshold = silence_threshold
        self.min_silence_duration = min_silence_duration
        
        logger.info("üîá Silence Detector: Inicializado")
        logger.info(f"   Threshold: {silence_threshold}")
        logger.info(f"   Dura√ß√£o m√≠nima: {min_silence_duration}s")
    
    def detect_silences(self, video_path: Path) -> List[SilenceSegment]:
        """
        Detecta segmentos de sil√™ncio no v√≠deo.
        
        Returns:
            Lista de SilenceSegment
        """
        logger.info(f"üîç Detectando sil√™ncios em: {video_path.name}")
        
        try:
            from moviepy.editor import VideoFileClip
            
            clip = VideoFileClip(str(video_path))
            
            if clip.audio is None:
                logger.warning("   ‚ö†Ô∏è V√≠deo n√£o tem √°udio")
                clip.close()
                return []
            
            # Extrair √°udio
            audio_array = clip.audio.to_soundarray(fps=22050)
            fps = 22050
            
            # Calcular volume por janela
            window_size = int(fps * 0.1)  # 100ms
            silences = []
            
            in_silence = False
            silence_start = 0
            
            for i in range(0, len(audio_array) - window_size, window_size):
                window = audio_array[i:i + window_size]
                volume = np.abs(window).mean()
                
                timestamp = i / fps
                
                if volume < self.silence_threshold:
                    if not in_silence:
                        in_silence = True
                        silence_start = timestamp
                else:
                    if in_silence:
                        silence_end = timestamp
                        duration = silence_end - silence_start
                        
                        if duration >= self.min_silence_duration:
                            silences.append(SilenceSegment(
                                start=silence_start,
                                end=silence_end,
                                duration=duration
                            ))
                        
                        in_silence = False
            
            # Verificar sil√™ncio no final
            if in_silence:
                silence_end = len(audio_array) / fps
                duration = silence_end - silence_start
                
                if duration >= self.min_silence_duration:
                    silences.append(SilenceSegment(
                        start=silence_start,
                        end=silence_end,
                        duration=duration
                    ))
            
            clip.close()
            
            logger.info(f"   ‚úÖ {len(silences)} sil√™ncios detectados")
            for s in silences[:5]:  # Mostrar primeiros 5
                logger.info(f"      {s.start:.1f}s - {s.end:.1f}s ({s.duration:.1f}s)")
            
            return silences
            
        except Exception as e:
            logger.error(f"   ‚ùå Erro ao detectar sil√™ncios: {e}")
            return []


class NarrationGenerator:
    """Gera texto de narra√ß√£o baseado no contexto."""
    
    def __init__(self):
        logger.info("üìù Narration Generator: Inicializado")
    
    def generate_narration_text(
        self,
        silence: SilenceSegment,
        transcription: List[Dict] = None,
        video_context: str = ""
    ) -> str:
        """
        Gera texto de narra√ß√£o para um segmento de sil√™ncio.
        
        Args:
            silence: Segmento de sil√™ncio
            transcription: Transcri√ß√£o do v√≠deo
            video_context: Contexto geral do v√≠deo
            
        Returns:
            Texto sugerido para narra√ß√£o
        """
        # Encontrar contexto antes e depois do sil√™ncio
        context_before = ""
        context_after = ""
        
        if transcription:
            for segment in transcription:
                seg_end = segment.get('end', 0)
                seg_start = segment.get('start', 0)
                text = segment.get('text', '')
                
                # Contexto antes
                if seg_end <= silence.start and seg_end > silence.start - 5:
                    context_before = text
                
                # Contexto depois
                if seg_start >= silence.end and seg_start < silence.end + 5:
                    context_after = text
                    break
        
        silence.context_before = context_before
        silence.context_after = context_after
        
        # Gerar narra√ß√£o baseada no contexto
        # TODO: Usar LLM para gerar narra√ß√£o mais inteligente
        
        if context_before and context_after:
            narration = f"Continuando sobre {context_before[:50]}..."
        elif context_before:
            narration = "Vamos ver mais sobre isso..."
        else:
            narration = ""
        
        silence.suggested_narration = narration
        return narration


class TextToSpeech:
    """Converte texto em √°udio de voz."""
    
    def __init__(self):
        self.available = False
        
        # Tentar importar bibliotecas de TTS
        try:
            import pyttsx3
            self.engine = pyttsx3.init()
            self.available = True
            self.method = 'pyttsx3'
            logger.info("üîä Text-to-Speech: Inicializado (pyttsx3)")
        except:
            try:
                from gtts import gTTS
                self.available = True
                self.method = 'gtts'
                logger.info("üîä Text-to-Speech: Inicializado (gTTS)")
            except:
                logger.warning("üîä Text-to-Speech: N√£o dispon√≠vel")
                self.method = None
    
    def synthesize(self, text: str, output_path: Path, language: str = 'pt-br') -> Optional[Path]:
        """
        Sintetiza texto em √°udio.
        
        Args:
            text: Texto para sintetizar
            output_path: Caminho do arquivo de sa√≠da
            language: Idioma
            
        Returns:
            Caminho do arquivo de √°udio ou None
        """
        if not self.available or not text:
            return None
        
        try:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            if self.method == 'pyttsx3':
                self.engine.save_to_file(text, str(output_path))
                self.engine.runAndWait()
                return output_path
            
            elif self.method == 'gtts':
                from gtts import gTTS
                tts = gTTS(text=text, lang=language[:2])
                tts.save(str(output_path))
                return output_path
            
        except Exception as e:
            logger.error(f"   ‚ùå Erro na s√≠ntese de voz: {e}")
            return None


class AutoNarrator:
    """Adiciona narra√ß√£o autom√°tica em sil√™ncios do v√≠deo."""
    
    def __init__(self):
        self.silence_detector = SilenceDetector()
        self.narration_generator = NarrationGenerator()
        self.tts = TextToSpeech()
        
        logger.info("üé§ Auto Narrator: Inicializado")
    
    def process_video(
        self,
        video_path: Path,
        output_path: Path,
        transcription: List[Dict] = None,
        add_narration: bool = True
    ) -> Path:
        """
        Processa v√≠deo adicionando narra√ß√£o em sil√™ncios.
        
        Args:
            video_path: V√≠deo de entrada
            output_path: V√≠deo de sa√≠da
            transcription: Transcri√ß√£o do v√≠deo
            add_narration: Se deve adicionar narra√ß√£o
            
        Returns:
            Caminho do v√≠deo processado
        """
        logger.info(f"üé¨ Processando narra√ß√£o autom√°tica: {video_path.name}")
        
        if not add_narration:
            return video_path
        
        # Detectar sil√™ncios
        silences = self.silence_detector.detect_silences(video_path)
        
        if not silences:
            logger.info("   ‚ÑπÔ∏è Nenhum sil√™ncio significativo detectado")
            return video_path
        
        # Gerar narra√ß√µes
        narrations = []
        for silence in silences:
            text = self.narration_generator.generate_narration_text(
                silence, transcription
            )
            if text:
                narrations.append((silence, text))
        
        if not narrations:
            logger.info("   ‚ÑπÔ∏è Nenhuma narra√ß√£o gerada")
            return video_path
        
        # Sintetizar e mixar
        if not self.tts.available:
            logger.warning("   ‚ö†Ô∏è TTS n√£o dispon√≠vel, pulando narra√ß√£o")
            return video_path
        
        # TODO: Implementar mixagem de √°udio
        logger.info(f"   ‚úÖ {len(narrations)} narra√ß√µes preparadas")
        
        return video_path
    
    def get_silence_segments(self, video_path: Path) -> List[SilenceSegment]:
        """Retorna segmentos de sil√™ncio do v√≠deo."""
        return self.silence_detector.detect_silences(video_path)


if __name__ == "__main__":
    narrator = AutoNarrator()
    print("Auto Narrator inicializado com sucesso!")
    print(f"TTS dispon√≠vel: {narrator.tts.available}")
