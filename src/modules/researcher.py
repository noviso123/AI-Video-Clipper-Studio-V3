"""
MÃ³dulo de Pesquisa e ExtraÃ§Ã£o Web (CÃ©rebro)
Usa Crawl4AI para capturar conteÃºdo de URLs e transformar em Markdown pronto para LLM.
"""
import os
import asyncio
from typing import Optional
from crawl4ai import AsyncWebCrawler
from ..core.logger import setup_logger

logger = setup_logger(__name__)

class ContentResearcher:
    """Extrai conteÃºdo de URLs de forma inteligente"""

    def __init__(self):
        self.temp_dir = os.path.join(os.getcwd(), 'temp', 'research')
        os.makedirs(self.temp_dir, exist_ok=True)

    async def crawl_url(self, url: str) -> str:
        """Processa uma URL e retorna o conteÃºdo em Markdown"""
        try:
            logger.info(f"ğŸŒ Iniciando Crawl: {url}")

            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url=url)

                if result.success:
                    logger.info(f"âœ… Crawl concluÃ­do: {len(result.markdown)} caracteres extraÃ­dos.")
                    return result.markdown
                else:
                    logger.error(f"âŒ Erro no Crawl: {result.error_message}")
                    return f"Erro no Crawl: {result.error_message}"

        except Exception as e:
            logger.error(f"âŒ Erro crÃ­tico no pesquisador: {e}")
            return f"Erro na pesquisa: {str(e)}"

    def run_crawl(self, url: str) -> str:
        """Wrapper sÃ­ncrono para facilitar a integraÃ§Ã£o com Flask se necessÃ¡rio"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            content = loop.run_until_complete(self.crawl_url(url))
            loop.close()
            return content
        except Exception as e:
            logger.error(f"Erro no loop de crawl: {e}")
            return str(e)

# Singleton
researcher = None
def get_researcher() -> ContentResearcher:
    global researcher
    if researcher is None:
        researcher = ContentResearcher()
    return researcher
