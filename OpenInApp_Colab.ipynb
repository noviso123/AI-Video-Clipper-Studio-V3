{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83c\udfac AI Video Clipper Studio - Google Colab Edition (GPU Turbo)\n",
                "\n",
                "Este notebook permite rodar o **AI Video Clipper** usando as GPUs do Google Colab (Tesla T4) para m\u00e1xima performance.\n",
                "\n",
                "### \ud83d\ude80 Vantagens desta vers\u00e3o:\n",
                "- **Renderiza\u00e7\u00e3o R\u00e1pida**: Usa GPU para transcrever 5x mais r\u00e1pido.\n",
                "- **Modelos Melhores**: Usa `whisper-large-v3` automaticamente para transcri\u00e7\u00e3o perfeita.\n",
                "- **Cache Inteligente**: Salva os modelos no seu Drive para n\u00e3o baixar toda vez.\n",
                "\n",
                "### \ud83d\udcdd Como usar:\n",
                "1. **Upload**: Fa\u00e7a upload da pasta do projeto para o Google Drive.\n",
                "2. **Conectar**: Rode a c\u00e9lula abaixo para conectar o Drive.\n",
                "3. **Start**: Instale e rode."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Conectar Google Drive e Configurar Cache\n",
                "from google.colab import drive\n",
                "import os\n",
                "import sys\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# @markdown --- \n",
                "# @markdown ** \ud83d\udc47 DIGITE O CAMINHO DO PROJETO NO DRIVE:**\n",
                "PROJECT_PATH = \"/content/drive/MyDrive/AI-Video-Clipper-Studio-V3\" # @param {type:\"string\"}\n",
                "\n",
                "# Cache de modelos para n\u00e3o baixar sempre\n",
                "MODELS_CACHE_DIR = os.path.join(PROJECT_PATH, \"models_cache\")\n",
                "os.makedirs(MODELS_CACHE_DIR, exist_ok=True)\n",
                "\n",
                "if os.path.exists(PROJECT_PATH):\n",
                "    os.chdir(PROJECT_PATH)\n",
                "    print(f\"\u2705 Diret\u00f3rio: {os.getcwd()}\")\n",
                "    print(f\"\ud83d\udcc2 Cache de Modelos: {MODELS_CACHE_DIR}\")\n",
                "else:\n",
                "    print(f\"\u274c Erro: O caminho '{PROJECT_PATH}' n\u00e3o existe.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Instala\u00e7\u00e3o Otimizada (GPU Support)\n",
                "\n",
                "print(\"\u23f3 Instalando FFmpeg e ImageMagick...\")\n",
                "!apt-get update -qq && apt-get install -y ffmpeg imagemagick -qq\n",
                "!sed -i '/<policy domain=\"path\" rights=\"none\" pattern=\"@*\"\\/>/d' /etc/ImageMagick-6/policy.xml\n",
                "\n",
                "print(\"\u23f3 Instalando Bibliotecas Python...\")\n",
                "!pip install -r colab_requirements.txt -q\n",
                "!pip install pyngrok==7.1.6 -q\n",
                "\n",
                "import torch\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"\ud83d\ude80 GPU ACELERADA: {torch.cuda.get_device_name(0)}\")\n",
                "    os.environ[\"WHISPER_MODEL\"] = \"large-v3\" # For\u00e7a modelo melhor\n",
                "    os.environ[\"VIDEO_QUALITY\"] = \"ultra\"\n",
                "    os.environ[\"USE_LOCAL_AI\"] = \"true\"\n",
                "else:\n",
                "    print(\"\u26a0\ufe0f GPU n\u00e3o detectada. Performance ser\u00e1 limitada.\")",
                "\n",
                "print(\"\ud83e\udde0 Instalando e Iniciando Ollama...\")\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "import subprocess\n",
                "import time\n",
                "\n",
                "# Iniciar Ollama em background\n",
                "subprocess.Popen([\"ollama\", \"serve\"])\n",
                "time.sleep(5) # Esperar servidor subir\n",
                "\n",
                "print(\"\ud83d\udce5 Baixando modelo Llama3 (pode demorar um pouco)...t\")\n",
                "!ollama pull llama3\n",
                "print(\"\u2705 Ollama Pronto!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Iniciar Servidor (Acesse o App)\n",
                "\n",
                "# @markdown Escolha como acessar o App:\n",
                "TUNNEL_METHOD = \"localtunnel\" # @param [\"localtunnel\", \"ngrok\"]\n",
                "NGROK_TOKEN = \"\" # @param {type:\"string\"}\n",
                "\n",
                "import threading\n",
                "import time\n",
                "from pyngrok import ngrok\n",
                "\n",
                "# Limpar processos antigos\n",
                "!pkill -f flask\n",
                "!pkill -f cloudflared\n",
                "!pkill -f lt\n",
                "\n",
                "def run_flask():\n",
                "    # Overrides de ambiente para garantir performance m\u00e1xima na nuvem\n",
                "    env = os.environ.copy()\n",
                "    env[\"FLASK_ENV\"] = \"production\"\n",
                "    !python app.py\n",
                "\n",
                "flask_thread = threading.Thread(target=run_flask)\n",
                "flask_thread.daemon = True\n",
                "flask_thread.start()\n",
                "\n",
                "print(\"\u23f3 Iniciando servidor...\")\n",
                "time.sleep(8)\n",
                "\n",
                "if TUNNEL_METHOD == \"ngrok\":\n",
                "    if not NGROK_TOKEN:\n",
                "        print(\"\u274c Token Ngrok necess\u00e1rio!\")\n",
                "    else:\n",
                "        ngrok.set_auth_token(NGROK_TOKEN)\n",
                "        public_url = ngrok.connect(5000)\n",
                "        print(f\"\\n\ud83c\udf89 SEU APP EST\u00c1 PRONTO: {public_url}\\n\")\n",
                "\n",
                "elif TUNNEL_METHOD == \"localtunnel\":\n",
                "    print(\"\ud83d\udd17 Gerando link p\u00fablico...\")\n",
                "    !npm install -g localtunnel > /dev/null 2>&1\n",
                "    print(\"\u26a0\ufe0f PASSWORD DO LOCALTUNNEL:\")\n",
                "    !curl -s ipv4.icanhazip.com\n",
                "    print(\"\\n\")\n",
                "    !lt --port 5000\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}